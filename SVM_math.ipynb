{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_math.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4J5rFyxyt0g",
        "colab_type": "text"
      },
      "source": [
        "## Support vector machine is highly preferred by many as it produces significant accuracy with less computation power. Support Vector Machine, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in classification objectives.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/300/0*9jEWNXTAao7phK-5.png)![alt text](https://miro.medium.com/max/300/0*0o8xIA4k3gXUDCFU.png)\n",
        "\n",
        "\n",
        "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
        " To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
        "\n",
        "\n",
        "\n",
        "###In simple words, The Support vector machine is a generalization of a classifier called Maximal margin classifier.\n",
        "\n",
        "\n",
        "\n",
        "There is some prerequisites to learn before knowing about Support Vector Machines...\n",
        "\n",
        "\n",
        "##1. Vectors:  \n",
        "\n",
        "A vector is a mathematical object(that has both a magnitude and a direction) that can be represented by an arrow.\n",
        "\n",
        "\n",
        "##The magnitude of a vector:\n",
        "\n",
        "The magnitude or length of a vector x is written by || x || , and is called its norm. we compute the norm of a vector x = (x1, x2, ...., xn) by using the **Euclidean norm**\n",
        " formula:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1000/1*ccWm-HBA85UnnFiWrFwvsg.png)\n",
        "                      \n",
        "In Python, computing the norm can easily be done by calling the norm function provided by the\n",
        "numpy module, as shown below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps6vPWH_6DM9",
        "colab_type": "code",
        "outputId": "0f139a9e-7ffc-4d37-c643-39322240c649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = [10,5]\n",
        "np.linalg.norm(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.180339887498949"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHDRBJjK6kC5",
        "colab_type": "text"
      },
      "source": [
        "##The Diretion of a Vector:\n",
        "\n",
        "it is a new vector for which the coordinates are the initial coordinates of our vector divided by its norm.The direction of a vector $x = $($u,$v) is the vector:   \n",
        "    ![alt text](https://raw.githubusercontent.com/kushalshm1/Machine-Learning/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/directionVector.jpg)\n",
        "\n",
        "\n",
        "**Note:** the norm of a direction vector is always 1.                                   \n",
        "\n",
        "\n",
        "Direction can be computed in Python using the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyVvBsUL6R7F",
        "colab_type": "code",
        "outputId": "7762df41-3e1f-444e-cbad-64cbc5990f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def direction(x):\n",
        "  return x/np.linalg.norm(x)\n",
        "direction(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.89442719, 0.4472136 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGJAfeDZ_SoF",
        "colab_type": "text"
      },
      "source": [
        "##The Dot product:\n",
        "The dot product is an operation performed on two vectors that returns a number. A number is sometimes called a scalar; that is why the dot product is also called a scalar product.\n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/kushalshm1/Machine-Learning/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/dotProduct.jpg)\n",
        "\n",
        "\n",
        "θ is the angle between two vectors By looking at this formula, we can see that the dot product is strongly influenced by the angle :\n",
        "\n",
        "* when theta = 0 degree, we have cos(theta) = 1 and x.y = ||x|| ||y||\n",
        "* when theta = 90 degree, we have cos(theta) = 0 and x.y = 0\n",
        "* when theta = 180 degree, we have cos(theta) = -1 and x.y =  - ||x|| ||y||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dgG8BQcB_yl",
        "colab_type": "text"
      },
      "source": [
        "##Linear Separability:\n",
        "\n",
        "Linearly separable data, Imagine you are a wine producer. You sell wine coming from two different production batches:\n",
        "* One high-end wine costing 1450rs a bottle.\n",
        "* One common wine costing 80rs a bottle.\n",
        "\n",
        "Recently, you started to receive complaints from clients who bought an expensive bottle. They\n",
        "claim that their bottle contains the cheap wine. This results in a major reputation loss for your\n",
        "company, and customers stop ordering your wine. Using alcohol-by-volume to classify wine You decide to find a way to distinguish the two wines. You know that one of them contains more alcohol than the other, so you open a few bottles, measure the alcohol concentration, and plot it.![alt text](https://newonlinecourses.science.psu.edu/onlinecourses/sites/stat508/files/lesson11/linearly_separable_01.png)\n",
        "\n",
        "you can clearly see that the expensive wine contains less alcohol than the cheap\n",
        "one. In fact, you can find a point that separates the data into two groups. This data is said to be\n",
        "linearly separable.\n",
        "\n",
        "data is linearly separable when:\n",
        "* In one dimension, you can find a point separating the data \n",
        "* In two dimensions, you can find a line separating the data \n",
        "* In three dimensions, you can find a plane separating the data\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMVy7wAHc1D4QxG5NnmE-2BojeQGhk5E7ztXtnoVVl7dbEJJy_Fg)![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTGWQ40dR5s-UEA1vpUKhnYIbr47FyXVuHUbAk0jbQ35A7IPSPP)\n",
        "\n",
        "\n",
        "##Hyperplanes: \n",
        "What do we use to separate the data when there are more than three dimensions? We use a hyperplane\n",
        "\n",
        "In geometry, a hyperplane is a subspace of one dimension less than its ambient space. It's mean, in a P-dimensional space a hyperplane is a flat affine subspace of dimension p-1, visually in a 2d space, the hyperplane will be a line and in a 3d space, it will be a flat plane.\n",
        "\n",
        "* Equation of Line:            \n",
        "![alt text](https://raw.githubusercontent.com/kushalshm1/Machine-Learning/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/lineEquation.jpg)\n",
        "\n",
        "* Equation of Hyperplane:   \n",
        "![alt text](https://raw.githubusercontent.com/kushalshm1/Machine-Learning/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/hyperplaneEquation.jpg)\n",
        "\n",
        "\n",
        "here m and c are both vectors of n dimension\n",
        "\n",
        "And in Linear Hard margin Support Vector Machines we seperate the data by two seperating hyperplane each of which is at outermost point of each class as well as their distance is maximum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5fcfBTbbvg2",
        "colab_type": "text"
      },
      "source": [
        "##Now move forward on Support Vector Machine algoritham\n",
        "\n",
        "The goal of a support vector machine is to nd the optimal separating hyperplane which maximizes the\n",
        "margin of the training data.\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://sandipanweb.files.wordpress.com/2018/04/svm_slack.png?w=676)\n",
        "\n",
        "\n",
        "\n",
        "*   Margin is perpendicular distance between two hyperplanes and this margin(R) is calculated as: \n",
        "\n",
        "       ![alt text](http://www.simpleimageresizer.com/_uploads/photos/ba774bfa/IMG_20190803_2249452_8_200x200.jpg)\n",
        "\n",
        "\n",
        "R is the margin which is to be maximized\n",
        "      \n",
        "**There are some assumptions:** \n",
        "\n",
        "max of 2/|| w|| is equal to min of || w || /2 \n",
        "\n",
        "2/|| w || is not convex function and we need to have a convex function in order to minimize easilly, so min of (||w||)^2  (**note:** here we are squaring to make it convex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauSJhjOlsy6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Hence our Primal Objective Function is this which is written above and we have to minimize it using mathematical optimization, But that is not an easy task to minimize it there are many more things to learn and do\n",
        "\n",
        "####What is Optimization?\n",
        "\n",
        "Optimization is the branch of Mathematics and Research in which we deal with problems of Minimizing and Maximizing function in a feasible way.\n",
        "Optimization Problem:\n",
        "\n",
        "An Optimization problem contains a function with some constraints which acts like bounding conditions of the value over whose extent that function will be minimized or maximized\n",
        "\n",
        "In this above problem of Male/Female voice classification we have formulated an Optimization Problem in which we have to minimize the margin function and our constraints are equations of Hyperplane.\n",
        "\n",
        "Hence, our optimization problem is,            ![alt text](https://github.com/kushalshm1/Machine-Learning/raw/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/PrimalProblem.jpg)\n",
        "\n",
        "\n",
        "After minimizing it we will get that minimum margin that classifies our data properly.\n",
        "\n",
        "This above written expression is our Primal problem\n",
        "But minimizing this Primal Problem is not easy so we will convert it into some other optimization problem that is easy to solve due to its resolved constraints and that Optimization problem is called Dual Problem\n",
        "\n",
        "####Duality:\n",
        "\n",
        "In order to make Dual Problem let's understand what is Duality first,\n",
        "\n",
        "So, in mathematical optimization theory, duality means that Optimization problems may be viewed from either of two perspectives, the Primal Problem and Dual Problem.\n",
        "####Duality Gap: ![alt text](https://raw.githubusercontent.com/kushalshm1/Machine-Learning/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/DualAndPrimalCurves.jpg)\n",
        "\n",
        "\n",
        "In the figure above, imagine that in our primal problem we are trying to minimize the function at the top of the graph. Its minimum is point P.\n",
        "\n",
        "If we search for Dual Function we could end up with the one at bottom of the graph whose maximum is point D.\n",
        "\n",
        "We define the value (P-D) and call it Duality Gap.\n",
        "\n",
        "and,\n",
        "\n",
        "if(P-D)>0 then the phenomenon is called as Weak Duality\n",
        "\n",
        "if(P-D) = 0 then the phenomenon is called Strong Duality\n",
        "\n",
        "Now, we will see method of Lagrange Multipliers to find Dual Problem, but before that some prerequisite knowledge is required so let's review those first.\n",
        "###Prerequisite Knowledge to learn Lagrange Multipliers Method\n",
        "####Gradients:\n",
        "\n",
        "A gradient of a function is a Vector field with the arrow pointing in the direction where the function is increasing. It is actually a partial derivative of function with respect to the variables. And it is always perpendicular to the curve of the function.\n",
        "\n",
        "\n",
        "let's take a sample optimization problem,\n",
        "\n",
        "\n",
        "   ![alt text](https://raw.githubusercontent.com/kushalshm1/Machine-Learning/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/SampleProblem.jpg)\n",
        "\n",
        "Let's plot these two equations f(x,y) and g(x,y)\n",
        "\n",
        "![alt text](https://github.com/kushalshm1/Machine-Learning/raw/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/IndividualContourPlots.jpg)\n",
        "\n",
        "Now, lets compute gradient of f(x,y) and g(x,y) both and let's see what happens: \n",
        "\n",
        "![alt text](https://github.com/kushalshm1/Machine-Learning/raw/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/CombinedContoursWithGradients.jpg)\n",
        "\n",
        "\n",
        "\n",
        "Here, you can see red arrows are gradients of f(x,y) and black arrows are gradients of g(x,y). And among all these arrow there exists some red and black arrows which are parallel to each other.\n",
        "\n",
        "And here comes a line which shares a parallel gradient with circle, \n",
        "\n",
        "![alt text](https://github.com/kushalshm1/Machine-Learning/raw/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/ParallelGradients.jpg)\n",
        "\n",
        "So, in the graph above we plot the line y = 1-x on the top of Objective function f(x,y).\n",
        "\n",
        "**Lagrange found that minimum of f(x,y) under the constraint g(x,y) = 0 is obtained when their gradients point in the same direction.**\n",
        "\n",
        "\n",
        "###Let's translate it Mathematically:\n",
        "\n",
        "When two gradients are parallel but not equal so there should exist a parameter that should make both equal in magnitude, so that is λ(Lambda) and this is called the famous LAGRANGE MULTIPLIER.\n",
        "\n",
        "And hence the equation becomes,\n",
        "\n",
        "Gradient of (f(x,y)) = λ * (Gradient of g(x,y))\n",
        "\n",
        "or\n",
        "\n",
        "Δ(f(x,y)) = λ * Δ(g(x,y))\n",
        "\n",
        "###What is λ?\n",
        "\n",
        "It is what we call a Lagrange Multiplier. Indeed, even it the two gradient vectors point in the same direction, they might not have the same length, there must be some factor λ allowing to transform on the the other.\n",
        "\n",
        "But how do we find points for which,\n",
        "\n",
        "Gradient of (f(x,y)) = λ * (Gradient of g(x,y)) exists,\n",
        "\n",
        "To get this we need to modify this a bit,\n",
        "\n",
        "Δ(f(x,y)) - λ * Δ(g(x,y)) = 0\n",
        "\n",
        "To make things little easier we are defining a function:\n",
        "\n",
        "![alt text](https://github.com/kushalshm1/Machine-Learning/raw/9c16bf1342a6a056196eb3ce4a7e60e043e6b801/SupportVectorMachines-Explanation/img/LagrangeFunctionZero.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq7TqmOq7hs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}