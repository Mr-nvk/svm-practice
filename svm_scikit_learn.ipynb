{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Support Vector machine with different kernels(linear,gaussian,polynomial) and also tune the various parameters such as C ,gamma and degree to find out the best performing model.**"},{"metadata":{},"cell_type":"markdown","source":"I am taking voice dataset of male and female"},{"metadata":{},"cell_type":"markdown","source":"**Importing Libraries**\n\n**Note:** here i am using %matplotlib inline, bcz the %matplotlib inline will make our plot outputs appear and be stored within the notebook.  "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/voice.csv')\ndf.head()","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n\n          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n\n     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meanfreq</th>\n      <th>sd</th>\n      <th>median</th>\n      <th>Q25</th>\n      <th>Q75</th>\n      <th>IQR</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sp.ent</th>\n      <th>sfm</th>\n      <th>...</th>\n      <th>centroid</th>\n      <th>meanfun</th>\n      <th>minfun</th>\n      <th>maxfun</th>\n      <th>meandom</th>\n      <th>mindom</th>\n      <th>maxdom</th>\n      <th>dfrange</th>\n      <th>modindx</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.059781</td>\n      <td>0.064241</td>\n      <td>0.032027</td>\n      <td>0.015071</td>\n      <td>0.090193</td>\n      <td>0.075122</td>\n      <td>12.863462</td>\n      <td>274.402906</td>\n      <td>0.893369</td>\n      <td>0.491918</td>\n      <td>...</td>\n      <td>0.059781</td>\n      <td>0.084279</td>\n      <td>0.015702</td>\n      <td>0.275862</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066009</td>\n      <td>0.067310</td>\n      <td>0.040229</td>\n      <td>0.019414</td>\n      <td>0.092666</td>\n      <td>0.073252</td>\n      <td>22.423285</td>\n      <td>634.613855</td>\n      <td>0.892193</td>\n      <td>0.513724</td>\n      <td>...</td>\n      <td>0.066009</td>\n      <td>0.107937</td>\n      <td>0.015826</td>\n      <td>0.250000</td>\n      <td>0.009014</td>\n      <td>0.007812</td>\n      <td>0.054688</td>\n      <td>0.046875</td>\n      <td>0.052632</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.077316</td>\n      <td>0.083829</td>\n      <td>0.036718</td>\n      <td>0.008701</td>\n      <td>0.131908</td>\n      <td>0.123207</td>\n      <td>30.757155</td>\n      <td>1024.927705</td>\n      <td>0.846389</td>\n      <td>0.478905</td>\n      <td>...</td>\n      <td>0.077316</td>\n      <td>0.098706</td>\n      <td>0.015656</td>\n      <td>0.271186</td>\n      <td>0.007990</td>\n      <td>0.007812</td>\n      <td>0.015625</td>\n      <td>0.007812</td>\n      <td>0.046512</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.151228</td>\n      <td>0.072111</td>\n      <td>0.158011</td>\n      <td>0.096582</td>\n      <td>0.207955</td>\n      <td>0.111374</td>\n      <td>1.232831</td>\n      <td>4.177296</td>\n      <td>0.963322</td>\n      <td>0.727232</td>\n      <td>...</td>\n      <td>0.151228</td>\n      <td>0.088965</td>\n      <td>0.017798</td>\n      <td>0.250000</td>\n      <td>0.201497</td>\n      <td>0.007812</td>\n      <td>0.562500</td>\n      <td>0.554688</td>\n      <td>0.247119</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.135120</td>\n      <td>0.079146</td>\n      <td>0.124656</td>\n      <td>0.078720</td>\n      <td>0.206045</td>\n      <td>0.127325</td>\n      <td>1.101174</td>\n      <td>4.333713</td>\n      <td>0.971955</td>\n      <td>0.783568</td>\n      <td>...</td>\n      <td>0.135120</td>\n      <td>0.106398</td>\n      <td>0.016931</td>\n      <td>0.266667</td>\n      <td>0.712812</td>\n      <td>0.007812</td>\n      <td>5.484375</td>\n      <td>5.476562</td>\n      <td>0.208274</td>\n      <td>male</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**correlation between each feature**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"          meanfreq        sd    median       Q25       Q75       IQR  \\\nmeanfreq  1.000000 -0.739039  0.925445  0.911416  0.740997 -0.627605   \nsd       -0.739039  1.000000 -0.562603 -0.846931 -0.161076  0.874660   \nmedian    0.925445 -0.562603  1.000000  0.774922  0.731849 -0.477352   \nQ25       0.911416 -0.846931  0.774922  1.000000  0.477140 -0.874189   \nQ75       0.740997 -0.161076  0.731849  0.477140  1.000000  0.009636   \nIQR      -0.627605  0.874660 -0.477352 -0.874189  0.009636  1.000000   \nskew     -0.322327  0.314597 -0.257407 -0.319475 -0.206339  0.249497   \nkurt     -0.316036  0.346241 -0.243382 -0.350182 -0.148881  0.316185   \nsp.ent   -0.601203  0.716620 -0.502005 -0.648126 -0.174905  0.640813   \nsfm      -0.784332  0.838086 -0.661690 -0.766875 -0.378198  0.663601   \nmode      0.687715 -0.529150  0.677433  0.591277  0.486857 -0.403764   \ncentroid  1.000000 -0.739039  0.925445  0.911416  0.740997 -0.627605   \nmeanfun   0.460844 -0.466281  0.414909  0.545035  0.155091 -0.534462   \nminfun    0.383937 -0.345609  0.337602  0.320994  0.258002 -0.222680   \nmaxfun    0.274004 -0.129662  0.251328  0.199841  0.285584 -0.069588   \nmeandom   0.536666 -0.482726  0.455943  0.467403  0.359181 -0.333362   \nmindom    0.229261 -0.357667  0.191169  0.302255 -0.023750 -0.357037   \nmaxdom    0.519528 -0.482278  0.438919  0.459683  0.335114 -0.337877   \ndfrange   0.515570 -0.475999  0.435621  0.454394  0.335648 -0.331563   \nmodindx  -0.216979  0.122660 -0.213298 -0.141377 -0.216475  0.041252   \n\n              skew      kurt    sp.ent       sfm      mode  centroid  \\\nmeanfreq -0.322327 -0.316036 -0.601203 -0.784332  0.687715  1.000000   \nsd        0.314597  0.346241  0.716620  0.838086 -0.529150 -0.739039   \nmedian   -0.257407 -0.243382 -0.502005 -0.661690  0.677433  0.925445   \nQ25      -0.319475 -0.350182 -0.648126 -0.766875  0.591277  0.911416   \nQ75      -0.206339 -0.148881 -0.174905 -0.378198  0.486857  0.740997   \nIQR       0.249497  0.316185  0.640813  0.663601 -0.403764 -0.627605   \nskew      1.000000  0.977020 -0.195459  0.079694 -0.434859 -0.322327   \nkurt      0.977020  1.000000 -0.127644  0.109884 -0.406722 -0.316036   \nsp.ent   -0.195459 -0.127644  1.000000  0.866411 -0.325298 -0.601203   \nsfm       0.079694  0.109884  0.866411  1.000000 -0.485913 -0.784332   \nmode     -0.434859 -0.406722 -0.325298 -0.485913  1.000000  0.687715   \ncentroid -0.322327 -0.316036 -0.601203 -0.784332  0.687715  1.000000   \nmeanfun  -0.167668 -0.194560 -0.513194 -0.421066  0.324771  0.460844   \nminfun   -0.216954 -0.203201 -0.305826 -0.362100  0.385467  0.383937   \nmaxfun   -0.080861 -0.045667 -0.120738 -0.192369  0.172329  0.274004   \nmeandom  -0.336848 -0.303234 -0.293562 -0.428442  0.491479  0.536666   \nmindom   -0.061608 -0.103313 -0.294869 -0.289593  0.198150  0.229261   \nmaxdom   -0.305651 -0.274500 -0.324253 -0.436649  0.477187  0.519528   \ndfrange  -0.304640 -0.272729 -0.319054 -0.431580  0.473775  0.515570   \nmodindx  -0.169325 -0.205539  0.198074  0.211477 -0.182344 -0.216979   \n\n           meanfun    minfun    maxfun   meandom    mindom    maxdom  \\\nmeanfreq  0.460844  0.383937  0.274004  0.536666  0.229261  0.519528   \nsd       -0.466281 -0.345609 -0.129662 -0.482726 -0.357667 -0.482278   \nmedian    0.414909  0.337602  0.251328  0.455943  0.191169  0.438919   \nQ25       0.545035  0.320994  0.199841  0.467403  0.302255  0.459683   \nQ75       0.155091  0.258002  0.285584  0.359181 -0.023750  0.335114   \nIQR      -0.534462 -0.222680 -0.069588 -0.333362 -0.357037 -0.337877   \nskew     -0.167668 -0.216954 -0.080861 -0.336848 -0.061608 -0.305651   \nkurt     -0.194560 -0.203201 -0.045667 -0.303234 -0.103313 -0.274500   \nsp.ent   -0.513194 -0.305826 -0.120738 -0.293562 -0.294869 -0.324253   \nsfm      -0.421066 -0.362100 -0.192369 -0.428442 -0.289593 -0.436649   \nmode      0.324771  0.385467  0.172329  0.491479  0.198150  0.477187   \ncentroid  0.460844  0.383937  0.274004  0.536666  0.229261  0.519528   \nmeanfun   1.000000  0.339387  0.311950  0.270840  0.162163  0.277982   \nminfun    0.339387  1.000000  0.213987  0.375979  0.082015  0.317860   \nmaxfun    0.311950  0.213987  1.000000  0.337553 -0.243426  0.355390   \nmeandom   0.270840  0.375979  0.337553  1.000000  0.099656  0.812838   \nmindom    0.162163  0.082015 -0.243426  0.099656  1.000000  0.026640   \nmaxdom    0.277982  0.317860  0.355390  0.812838  0.026640  1.000000   \ndfrange   0.275154  0.316486  0.359880  0.811304  0.008666  0.999838   \nmodindx  -0.054858  0.002042 -0.363029 -0.180954  0.200212 -0.425531   \n\n           dfrange   modindx  \nmeanfreq  0.515570 -0.216979  \nsd       -0.475999  0.122660  \nmedian    0.435621 -0.213298  \nQ25       0.454394 -0.141377  \nQ75       0.335648 -0.216475  \nIQR      -0.331563  0.041252  \nskew     -0.304640 -0.169325  \nkurt     -0.272729 -0.205539  \nsp.ent   -0.319054  0.198074  \nsfm      -0.431580  0.211477  \nmode      0.473775 -0.182344  \ncentroid  0.515570 -0.216979  \nmeanfun   0.275154 -0.054858  \nminfun    0.316486  0.002042  \nmaxfun    0.359880 -0.363029  \nmeandom   0.811304 -0.180954  \nmindom    0.008666  0.200212  \nmaxdom    0.999838 -0.425531  \ndfrange   1.000000 -0.429266  \nmodindx  -0.429266  1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meanfreq</th>\n      <th>sd</th>\n      <th>median</th>\n      <th>Q25</th>\n      <th>Q75</th>\n      <th>IQR</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sp.ent</th>\n      <th>sfm</th>\n      <th>mode</th>\n      <th>centroid</th>\n      <th>meanfun</th>\n      <th>minfun</th>\n      <th>maxfun</th>\n      <th>meandom</th>\n      <th>mindom</th>\n      <th>maxdom</th>\n      <th>dfrange</th>\n      <th>modindx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>meanfreq</th>\n      <td>1.000000</td>\n      <td>-0.739039</td>\n      <td>0.925445</td>\n      <td>0.911416</td>\n      <td>0.740997</td>\n      <td>-0.627605</td>\n      <td>-0.322327</td>\n      <td>-0.316036</td>\n      <td>-0.601203</td>\n      <td>-0.784332</td>\n      <td>0.687715</td>\n      <td>1.000000</td>\n      <td>0.460844</td>\n      <td>0.383937</td>\n      <td>0.274004</td>\n      <td>0.536666</td>\n      <td>0.229261</td>\n      <td>0.519528</td>\n      <td>0.515570</td>\n      <td>-0.216979</td>\n    </tr>\n    <tr>\n      <th>sd</th>\n      <td>-0.739039</td>\n      <td>1.000000</td>\n      <td>-0.562603</td>\n      <td>-0.846931</td>\n      <td>-0.161076</td>\n      <td>0.874660</td>\n      <td>0.314597</td>\n      <td>0.346241</td>\n      <td>0.716620</td>\n      <td>0.838086</td>\n      <td>-0.529150</td>\n      <td>-0.739039</td>\n      <td>-0.466281</td>\n      <td>-0.345609</td>\n      <td>-0.129662</td>\n      <td>-0.482726</td>\n      <td>-0.357667</td>\n      <td>-0.482278</td>\n      <td>-0.475999</td>\n      <td>0.122660</td>\n    </tr>\n    <tr>\n      <th>median</th>\n      <td>0.925445</td>\n      <td>-0.562603</td>\n      <td>1.000000</td>\n      <td>0.774922</td>\n      <td>0.731849</td>\n      <td>-0.477352</td>\n      <td>-0.257407</td>\n      <td>-0.243382</td>\n      <td>-0.502005</td>\n      <td>-0.661690</td>\n      <td>0.677433</td>\n      <td>0.925445</td>\n      <td>0.414909</td>\n      <td>0.337602</td>\n      <td>0.251328</td>\n      <td>0.455943</td>\n      <td>0.191169</td>\n      <td>0.438919</td>\n      <td>0.435621</td>\n      <td>-0.213298</td>\n    </tr>\n    <tr>\n      <th>Q25</th>\n      <td>0.911416</td>\n      <td>-0.846931</td>\n      <td>0.774922</td>\n      <td>1.000000</td>\n      <td>0.477140</td>\n      <td>-0.874189</td>\n      <td>-0.319475</td>\n      <td>-0.350182</td>\n      <td>-0.648126</td>\n      <td>-0.766875</td>\n      <td>0.591277</td>\n      <td>0.911416</td>\n      <td>0.545035</td>\n      <td>0.320994</td>\n      <td>0.199841</td>\n      <td>0.467403</td>\n      <td>0.302255</td>\n      <td>0.459683</td>\n      <td>0.454394</td>\n      <td>-0.141377</td>\n    </tr>\n    <tr>\n      <th>Q75</th>\n      <td>0.740997</td>\n      <td>-0.161076</td>\n      <td>0.731849</td>\n      <td>0.477140</td>\n      <td>1.000000</td>\n      <td>0.009636</td>\n      <td>-0.206339</td>\n      <td>-0.148881</td>\n      <td>-0.174905</td>\n      <td>-0.378198</td>\n      <td>0.486857</td>\n      <td>0.740997</td>\n      <td>0.155091</td>\n      <td>0.258002</td>\n      <td>0.285584</td>\n      <td>0.359181</td>\n      <td>-0.023750</td>\n      <td>0.335114</td>\n      <td>0.335648</td>\n      <td>-0.216475</td>\n    </tr>\n    <tr>\n      <th>IQR</th>\n      <td>-0.627605</td>\n      <td>0.874660</td>\n      <td>-0.477352</td>\n      <td>-0.874189</td>\n      <td>0.009636</td>\n      <td>1.000000</td>\n      <td>0.249497</td>\n      <td>0.316185</td>\n      <td>0.640813</td>\n      <td>0.663601</td>\n      <td>-0.403764</td>\n      <td>-0.627605</td>\n      <td>-0.534462</td>\n      <td>-0.222680</td>\n      <td>-0.069588</td>\n      <td>-0.333362</td>\n      <td>-0.357037</td>\n      <td>-0.337877</td>\n      <td>-0.331563</td>\n      <td>0.041252</td>\n    </tr>\n    <tr>\n      <th>skew</th>\n      <td>-0.322327</td>\n      <td>0.314597</td>\n      <td>-0.257407</td>\n      <td>-0.319475</td>\n      <td>-0.206339</td>\n      <td>0.249497</td>\n      <td>1.000000</td>\n      <td>0.977020</td>\n      <td>-0.195459</td>\n      <td>0.079694</td>\n      <td>-0.434859</td>\n      <td>-0.322327</td>\n      <td>-0.167668</td>\n      <td>-0.216954</td>\n      <td>-0.080861</td>\n      <td>-0.336848</td>\n      <td>-0.061608</td>\n      <td>-0.305651</td>\n      <td>-0.304640</td>\n      <td>-0.169325</td>\n    </tr>\n    <tr>\n      <th>kurt</th>\n      <td>-0.316036</td>\n      <td>0.346241</td>\n      <td>-0.243382</td>\n      <td>-0.350182</td>\n      <td>-0.148881</td>\n      <td>0.316185</td>\n      <td>0.977020</td>\n      <td>1.000000</td>\n      <td>-0.127644</td>\n      <td>0.109884</td>\n      <td>-0.406722</td>\n      <td>-0.316036</td>\n      <td>-0.194560</td>\n      <td>-0.203201</td>\n      <td>-0.045667</td>\n      <td>-0.303234</td>\n      <td>-0.103313</td>\n      <td>-0.274500</td>\n      <td>-0.272729</td>\n      <td>-0.205539</td>\n    </tr>\n    <tr>\n      <th>sp.ent</th>\n      <td>-0.601203</td>\n      <td>0.716620</td>\n      <td>-0.502005</td>\n      <td>-0.648126</td>\n      <td>-0.174905</td>\n      <td>0.640813</td>\n      <td>-0.195459</td>\n      <td>-0.127644</td>\n      <td>1.000000</td>\n      <td>0.866411</td>\n      <td>-0.325298</td>\n      <td>-0.601203</td>\n      <td>-0.513194</td>\n      <td>-0.305826</td>\n      <td>-0.120738</td>\n      <td>-0.293562</td>\n      <td>-0.294869</td>\n      <td>-0.324253</td>\n      <td>-0.319054</td>\n      <td>0.198074</td>\n    </tr>\n    <tr>\n      <th>sfm</th>\n      <td>-0.784332</td>\n      <td>0.838086</td>\n      <td>-0.661690</td>\n      <td>-0.766875</td>\n      <td>-0.378198</td>\n      <td>0.663601</td>\n      <td>0.079694</td>\n      <td>0.109884</td>\n      <td>0.866411</td>\n      <td>1.000000</td>\n      <td>-0.485913</td>\n      <td>-0.784332</td>\n      <td>-0.421066</td>\n      <td>-0.362100</td>\n      <td>-0.192369</td>\n      <td>-0.428442</td>\n      <td>-0.289593</td>\n      <td>-0.436649</td>\n      <td>-0.431580</td>\n      <td>0.211477</td>\n    </tr>\n    <tr>\n      <th>mode</th>\n      <td>0.687715</td>\n      <td>-0.529150</td>\n      <td>0.677433</td>\n      <td>0.591277</td>\n      <td>0.486857</td>\n      <td>-0.403764</td>\n      <td>-0.434859</td>\n      <td>-0.406722</td>\n      <td>-0.325298</td>\n      <td>-0.485913</td>\n      <td>1.000000</td>\n      <td>0.687715</td>\n      <td>0.324771</td>\n      <td>0.385467</td>\n      <td>0.172329</td>\n      <td>0.491479</td>\n      <td>0.198150</td>\n      <td>0.477187</td>\n      <td>0.473775</td>\n      <td>-0.182344</td>\n    </tr>\n    <tr>\n      <th>centroid</th>\n      <td>1.000000</td>\n      <td>-0.739039</td>\n      <td>0.925445</td>\n      <td>0.911416</td>\n      <td>0.740997</td>\n      <td>-0.627605</td>\n      <td>-0.322327</td>\n      <td>-0.316036</td>\n      <td>-0.601203</td>\n      <td>-0.784332</td>\n      <td>0.687715</td>\n      <td>1.000000</td>\n      <td>0.460844</td>\n      <td>0.383937</td>\n      <td>0.274004</td>\n      <td>0.536666</td>\n      <td>0.229261</td>\n      <td>0.519528</td>\n      <td>0.515570</td>\n      <td>-0.216979</td>\n    </tr>\n    <tr>\n      <th>meanfun</th>\n      <td>0.460844</td>\n      <td>-0.466281</td>\n      <td>0.414909</td>\n      <td>0.545035</td>\n      <td>0.155091</td>\n      <td>-0.534462</td>\n      <td>-0.167668</td>\n      <td>-0.194560</td>\n      <td>-0.513194</td>\n      <td>-0.421066</td>\n      <td>0.324771</td>\n      <td>0.460844</td>\n      <td>1.000000</td>\n      <td>0.339387</td>\n      <td>0.311950</td>\n      <td>0.270840</td>\n      <td>0.162163</td>\n      <td>0.277982</td>\n      <td>0.275154</td>\n      <td>-0.054858</td>\n    </tr>\n    <tr>\n      <th>minfun</th>\n      <td>0.383937</td>\n      <td>-0.345609</td>\n      <td>0.337602</td>\n      <td>0.320994</td>\n      <td>0.258002</td>\n      <td>-0.222680</td>\n      <td>-0.216954</td>\n      <td>-0.203201</td>\n      <td>-0.305826</td>\n      <td>-0.362100</td>\n      <td>0.385467</td>\n      <td>0.383937</td>\n      <td>0.339387</td>\n      <td>1.000000</td>\n      <td>0.213987</td>\n      <td>0.375979</td>\n      <td>0.082015</td>\n      <td>0.317860</td>\n      <td>0.316486</td>\n      <td>0.002042</td>\n    </tr>\n    <tr>\n      <th>maxfun</th>\n      <td>0.274004</td>\n      <td>-0.129662</td>\n      <td>0.251328</td>\n      <td>0.199841</td>\n      <td>0.285584</td>\n      <td>-0.069588</td>\n      <td>-0.080861</td>\n      <td>-0.045667</td>\n      <td>-0.120738</td>\n      <td>-0.192369</td>\n      <td>0.172329</td>\n      <td>0.274004</td>\n      <td>0.311950</td>\n      <td>0.213987</td>\n      <td>1.000000</td>\n      <td>0.337553</td>\n      <td>-0.243426</td>\n      <td>0.355390</td>\n      <td>0.359880</td>\n      <td>-0.363029</td>\n    </tr>\n    <tr>\n      <th>meandom</th>\n      <td>0.536666</td>\n      <td>-0.482726</td>\n      <td>0.455943</td>\n      <td>0.467403</td>\n      <td>0.359181</td>\n      <td>-0.333362</td>\n      <td>-0.336848</td>\n      <td>-0.303234</td>\n      <td>-0.293562</td>\n      <td>-0.428442</td>\n      <td>0.491479</td>\n      <td>0.536666</td>\n      <td>0.270840</td>\n      <td>0.375979</td>\n      <td>0.337553</td>\n      <td>1.000000</td>\n      <td>0.099656</td>\n      <td>0.812838</td>\n      <td>0.811304</td>\n      <td>-0.180954</td>\n    </tr>\n    <tr>\n      <th>mindom</th>\n      <td>0.229261</td>\n      <td>-0.357667</td>\n      <td>0.191169</td>\n      <td>0.302255</td>\n      <td>-0.023750</td>\n      <td>-0.357037</td>\n      <td>-0.061608</td>\n      <td>-0.103313</td>\n      <td>-0.294869</td>\n      <td>-0.289593</td>\n      <td>0.198150</td>\n      <td>0.229261</td>\n      <td>0.162163</td>\n      <td>0.082015</td>\n      <td>-0.243426</td>\n      <td>0.099656</td>\n      <td>1.000000</td>\n      <td>0.026640</td>\n      <td>0.008666</td>\n      <td>0.200212</td>\n    </tr>\n    <tr>\n      <th>maxdom</th>\n      <td>0.519528</td>\n      <td>-0.482278</td>\n      <td>0.438919</td>\n      <td>0.459683</td>\n      <td>0.335114</td>\n      <td>-0.337877</td>\n      <td>-0.305651</td>\n      <td>-0.274500</td>\n      <td>-0.324253</td>\n      <td>-0.436649</td>\n      <td>0.477187</td>\n      <td>0.519528</td>\n      <td>0.277982</td>\n      <td>0.317860</td>\n      <td>0.355390</td>\n      <td>0.812838</td>\n      <td>0.026640</td>\n      <td>1.000000</td>\n      <td>0.999838</td>\n      <td>-0.425531</td>\n    </tr>\n    <tr>\n      <th>dfrange</th>\n      <td>0.515570</td>\n      <td>-0.475999</td>\n      <td>0.435621</td>\n      <td>0.454394</td>\n      <td>0.335648</td>\n      <td>-0.331563</td>\n      <td>-0.304640</td>\n      <td>-0.272729</td>\n      <td>-0.319054</td>\n      <td>-0.431580</td>\n      <td>0.473775</td>\n      <td>0.515570</td>\n      <td>0.275154</td>\n      <td>0.316486</td>\n      <td>0.359880</td>\n      <td>0.811304</td>\n      <td>0.008666</td>\n      <td>0.999838</td>\n      <td>1.000000</td>\n      <td>-0.429266</td>\n    </tr>\n    <tr>\n      <th>modindx</th>\n      <td>-0.216979</td>\n      <td>0.122660</td>\n      <td>-0.213298</td>\n      <td>-0.141377</td>\n      <td>-0.216475</td>\n      <td>0.041252</td>\n      <td>-0.169325</td>\n      <td>-0.205539</td>\n      <td>0.198074</td>\n      <td>0.211477</td>\n      <td>-0.182344</td>\n      <td>-0.216979</td>\n      <td>-0.054858</td>\n      <td>0.002042</td>\n      <td>-0.363029</td>\n      <td>-0.180954</td>\n      <td>0.200212</td>\n      <td>-0.425531</td>\n      <td>-0.429266</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Checking whether there is any null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"meanfreq    0\nsd          0\nmedian      0\nQ25         0\nQ75         0\nIQR         0\nskew        0\nkurt        0\nsp.ent      0\nsfm         0\nmode        0\ncentroid    0\nmeanfun     0\nminfun      0\nmaxfun      0\nmeandom     0\nmindom      0\nmaxdom      0\ndfrange     0\nmodindx     0\nlabel       0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"(3168, 21)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of labels: \",df.shape[0])\nprint(\"Total number of male: \",df[df[\"label\"] == \"male\"].shape[0])\nprint(\"Total number of male: \",df[df[\"label\"] == \"female\"].shape[0])","execution_count":38,"outputs":[{"output_type":"stream","text":"Total number of labels:  3168\nTotal number of male:  1584\nTotal number of male:  1584\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = df[\"label\"]\ndf.drop([\"label\"], axis = 1, inplace=True)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df\nX.shape","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"(3168, 20)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Converting string value to int type for labels**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = Y.replace(\"male\",1)\nY = Y.replace(\"female\",0)\n# we can also do it with scikit-learn using below code\n# from sklearn.preprocessing import LabelEncoder\n# y=df.iloc[:,-1]\n# gender_encoder = LabelEncoder()\n# y = gender_encoder.fit_transform(y)\n","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Standardisation**\n\nStandardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). It is useful to standardize attributes for a model. Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the data to be between -1 and 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spliting dataset into training and testing dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"(2534, 20)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**SVM with default hyperparameter.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nSVC = SVC()\nSVC.fit(X_train,y_train)\ny_pred = SVC.predict(X_test)\nfrom sklearn import metrics\nprint(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))","execution_count":45,"outputs":[{"output_type":"stream","text":"Accuracy:  0.9763406940063092\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**RBF kernel**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nSVC = SVC(kernel=\"rbf\")\nSVC.fit(X_train,y_train)\ny_pred = SVC.predict(X_test)\nfrom sklearn import metrics\nprint(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))","execution_count":46,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"Accuracy:  0.9763406940063092\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Ploynomial kernel**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nSVC = SVC(kernel=\"poly\")\nSVC.fit(X_train,y_train)\ny_pred = SVC.predict(X_test)\nfrom sklearn import metrics\nprint(\"Accuracy: \", metrics.accuracy_score(y_test,y_pred))","execution_count":47,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n","name":"stderr"},{"output_type":"stream","text":"Accuracy:  0.9589905362776026\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"\nTaking all the values of C and checking out the accuracy score with kernel as linear.\n\nThe C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.\n\nThus for a very large values we can cause overfitting of the model and for a very small value of C we can cause underfitting.Thus the value of C must be chosen in such a manner that it generalised the unseen data well\n\n\n\nTaking kernel as rbf and taking different values gamma\n\nTechnically, the gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nc_range = list(range(1,30))\nscores = {}\ng_range = [0.1,0.01, 0.001, 0.0001]\nfor g in g_range:\n    for  c in c_range:\n        from sklearn.svm import SVC\n        SVC = SVC(kernel=\"rbf\",C=c,gamma=g)\n        SVC.fit(X_train,y_train)\n        y_pred = SVC.predict(X_test)\n        from sklearn import metrics\n        scores[metrics.accuracy_score(y_test,y_pred)] = (c,g)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores.keys())","execution_count":49,"outputs":[{"output_type":"stream","text":"dict_keys([0.9779179810725552, 0.9810725552050473, 0.9794952681388013, 0.9763406940063092, 0.9747634069400631, 0.973186119873817, 0.9668769716088328, 0.9716088328075709, 0.9574132492113565, 0.9589905362776026, 0.9621451104100947, 0.9684542586750788, 0.9700315457413249, 0.8801261829652997, 0.8927444794952681, 0.9037854889589906, 0.9132492113564669, 0.919558359621451, 0.9290220820189274, 0.9400630914826499, 0.9479495268138801, 0.9526813880126183, 0.9542586750788643, 0.9558359621451105, 0.9605678233438486, 0.9637223974763407])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"maximum_acc = (max(scores.keys()))\nprint(\"Maximum accuracy is:\",maximum_acc)\nprint(\"values of c and g for maximum accuracy :\",scores[maximum_acc])","execution_count":50,"outputs":[{"output_type":"stream","text":"Maximum accuracy is: 0.9810725552050473\nvalues of c and g for maximum accuracy : (2, 0.1)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**So we achived maximum accuracy: 98.1% with c=2 and gamma=0.1**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}